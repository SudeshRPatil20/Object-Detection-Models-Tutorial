{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1698392,"sourceType":"datasetVersion","datasetId":991734}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-19T12:27:06.597704Z","iopub.execute_input":"2025-09-19T12:27:06.597909Z","iopub.status.idle":"2025-09-19T12:27:06.602299Z","shell.execute_reply.started":"2025-09-19T12:27:06.597882Z","shell.execute_reply":"2025-09-19T12:27:06.601650Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nfrom torch import nn\nimport torch\nimport time\n\nfrom PIL import Image\nfrom torchvision import transforms\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import Adam, AdamW, SGD\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:36:41.640613Z","iopub.execute_input":"2025-09-08T12:36:41.640945Z","iopub.status.idle":"2025-09-08T12:36:50.384770Z","shell.execute_reply.started":"2025-09-08T12:36:41.640918Z","shell.execute_reply":"2025-09-08T12:36:50.383983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom PIL import Image\n\nclass ImageSegmentation(Dataset):\n    def __init__(self, img_dir, mask_dir):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n\n        self.img_transform = transforms.Compose([\n            transforms.Resize((512, 512)),\n            transforms.ToTensor()\n        ])\n        self.mask_transform = transforms.Compose([\n            transforms.Resize((512, 512), interpolation=Image.NEAREST),\n            transforms.ToTensor()\n        ])\n\n        valid_extensions = {\".jpg\", \".jpeg\", \".png\"}\n        self.images = [f for f in os.listdir(img_dir) if os.path.splitext(f)[1].lower() in valid_extensions]\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image_path = os.path.join(self.img_dir, self.images[idx])\n        name, _ = os.path.splitext(self.images[idx])\n        mask_path = os.path.join(self.mask_dir, f\"{name}.png\")\n\n        image = Image.open(image_path).convert(\"RGB\")\n        mask = Image.open(mask_path).convert(\"L\")\n\n        image = self.img_transform(image)\n        mask = self.mask_transform(mask)\n\n        mask = (mask > 0.5).float()\n\n        return image, mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:36:50.386358Z","iopub.execute_input":"2025-09-08T12:36:50.386641Z","iopub.status.idle":"2025-09-08T12:36:50.393179Z","shell.execute_reply.started":"2025-09-08T12:36:50.386625Z","shell.execute_reply":"2025-09-08T12:36:50.392453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_dataloader(img_dir, mask_dir, batch_size=2, shuffle=True):\n    dataset = ImageSegmentation(img_dir, mask_dir)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:36:50.396420Z","iopub.execute_input":"2025-09-08T12:36:50.396851Z","iopub.status.idle":"2025-09-08T12:36:50.413706Z","shell.execute_reply.started":"2025-09-08T12:36:50.396822Z","shell.execute_reply":"2025-09-08T12:36:50.413141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv_op = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv_op(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:36:50.414403Z","iopub.execute_input":"2025-09-08T12:36:50.414934Z","iopub.status.idle":"2025-09-08T12:36:50.427016Z","shell.execute_reply.started":"2025-09-08T12:36:50.414909Z","shell.execute_reply":"2025-09-08T12:36:50.426518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DownSample(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = DoubleConv(in_channels, out_channels)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        down = self.conv(x)\n        p = self.pool(down)\n        return down, p","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:36:50.428727Z","iopub.execute_input":"2025-09-08T12:36:50.429125Z","iopub.status.idle":"2025-09-08T12:36:50.442059Z","shell.execute_reply.started":"2025-09-08T12:36:50.429083Z","shell.execute_reply":"2025-09-08T12:36:50.441414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass UpSample(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n        self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n\n        # Pad in case of odd dimensions mismatch\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:36:50.442666Z","iopub.execute_input":"2025-09-08T12:36:50.442840Z","iopub.status.idle":"2025-09-08T12:36:50.454165Z","shell.execute_reply.started":"2025-09-08T12:36:50.442827Z","shell.execute_reply":"2025-09-08T12:36:50.453448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        self.down_convolution_1 = DownSample(in_channels, 64)\n        self.down_convolution_2 = DownSample(64, 128)\n        self.down_convolution_3 = DownSample(128, 256)\n        self.down_convolution_4 = DownSample(256, 512)\n\n        self.bottleneck = DoubleConv(512, 1024)\n\n        self.up_convolution_1 = UpSample(1024, 512)\n        self.up_convolution_2 = UpSample(512, 256)\n        self.up_convolution_3 = UpSample(256, 128)\n        self.up_convolution_4 = UpSample(128, 64)\n\n        # final output conv\n        self.out = nn.Conv2d(64, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        down_1, p1 = self.down_convolution_1(x)\n        down_2, p2 = self.down_convolution_2(p1)\n        down_3, p3 = self.down_convolution_3(p2)\n        down_4, p4 = self.down_convolution_4(p3)\n\n        b = self.bottleneck(p4)\n\n        up_1 = self.up_convolution_1(b, down_4)\n        up_2 = self.up_convolution_2(up_1, down_3)\n        up_3 = self.up_convolution_3(up_2, down_2)\n        up_4 = self.up_convolution_4(up_3, down_1)\n\n        out = self.out(up_4)\n\n        return out   # logits (apply sigmoid/softmax outside depending on loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:36:50.454903Z","iopub.execute_input":"2025-09-08T12:36:50.455173Z","iopub.status.idle":"2025-09-08T12:36:50.466347Z","shell.execute_reply.started":"2025-09-08T12:36:50.455150Z","shell.execute_reply":"2025-09-08T12:36:50.465745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, smooth=1e-6):\n        super(DiceLoss, self).__init__()\n        self.smooth=smooth\n\n    def forward(self, inputs, targets):\n        inputs=inputs.view(-1)\n        targets=targets.view(-1)\n\n        intersection=(inputs * targets).sum()\n        dice_score=(2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n\n        return 1 - dice_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:36:50.467187Z","iopub.execute_input":"2025-09-08T12:36:50.467480Z","iopub.status.idle":"2025-09-08T12:36:50.480216Z","shell.execute_reply.started":"2025-09-08T12:36:50.467456Z","shell.execute_reply":"2025-09-08T12:36:50.479692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BCEWithDiceLoss(nn.Module):\n    def __init__(self, smooth=1e-6):\n        super(BCEWithDiceLoss, self).__init__()\n        self.bce = nn.BCEWithLogitsLoss()\n        self.dice = DiceLoss()\n\n    def forward(self, inputs, targets):\n        bce_loss = self.bce(inputs, targets)\n        dice_loss = self.dice(inputs, targets)\n        return 0.5*bce_loss + dice_loss\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:36:50.718392Z","iopub.execute_input":"2025-09-08T12:36:50.718590Z","iopub.status.idle":"2025-09-08T12:36:50.722886Z","shell.execute_reply.started":"2025-09-08T12:36:50.718575Z","shell.execute_reply":"2025-09-08T12:36:50.722324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport os\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim import Adam  # you can use SGD if you prefer\n\ndef train(model, dataloader, epochs=10, lr=0.001, save_path=\"unet_model\", load_path=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Load checkpoint if available\n    if load_path and os.path.exists(load_path):\n        print(f\"Loading model weights from {load_path}\")\n        model.load_state_dict(torch.load(load_path, map_location=device))\n    else:\n        print(\"No checkpoint found, training from scratch\")\n\n    print(f\"Using device: {device}\")\n    model.to(device)\n\n    # Loss & optimizer\n    criterion = BCEWithDiceLoss()   # keep your custom loss\n    optimizer = Adam(model.parameters(), lr=lr)\n    scaler = GradScaler()  # for mixed precision\n\n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0.0\n\n        for images, mask in dataloader:\n            images, mask = images.to(device), mask.to(device)\n\n            optimizer.zero_grad()\n\n            # mixed precision training\n            with autocast():\n                output = model(images)\n                loss = criterion(output, mask)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            epoch_loss += loss.item()\n\n        avg_loss = epoch_loss / len(dataloader)\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, LR: {lr}\")\n\n        # save every 10 epochs\n        if (epoch + 1) % 10 == 0:\n            torch.save(model.state_dict(), f\"{save_path}_epoch{epoch+1}.pth\")\n\n        # free cached GPU memory\n        torch.cuda.empty_cache()\n\n    # final save\n    torch.save(model.state_dict(), f\"{save_path}_final.pth\")\n    print(f\"Model saved to {save_path}_final.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:40:22.875331Z","iopub.execute_input":"2025-09-08T12:40:22.875595Z","iopub.status.idle":"2025-09-08T12:40:22.885911Z","shell.execute_reply.started":"2025-09-08T12:40:22.875577Z","shell.execute_reply":"2025-09-08T12:40:22.885166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = ImageSegmentation(\n    img_dir=\"/kaggle/input/buildings-dataset/src\",\n    mask_dir=\"/kaggle/input/buildings-dataset/label\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:48:32.875757Z","iopub.execute_input":"2025-09-08T12:48:32.876245Z","iopub.status.idle":"2025-09-08T12:48:32.886691Z","shell.execute_reply.started":"2025-09-08T12:48:32.876222Z","shell.execute_reply":"2025-09-08T12:48:32.886107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:49:36.327835Z","iopub.execute_input":"2025-09-08T12:49:36.328136Z","iopub.status.idle":"2025-09-08T12:49:36.331837Z","shell.execute_reply.started":"2025-09-08T12:49:36.328084Z","shell.execute_reply":"2025-09-08T12:49:36.331347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = UNet(in_channels=3, num_classes=1)\ntrain(model, dataloader, epochs=1, lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:57:21.309577Z","iopub.status.idle":"2025-09-08T12:57:21.309777Z","shell.execute_reply.started":"2025-09-08T12:57:21.309678Z","shell.execute_reply":"2025-09-08T12:57:21.309688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataloader = get_dataloader(\"/kaggle/input/buildings-dataset/src\", \"/kaggle/input/buildings-dataset/label\", batch_size=1, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:40:25.553717Z","iopub.execute_input":"2025-09-08T12:40:25.554562Z","iopub.status.idle":"2025-09-08T12:40:25.565752Z","shell.execute_reply.started":"2025-09-08T12:40:25.554527Z","shell.execute_reply":"2025-09-08T12:40:25.565152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model=UNet(in_channels=3, num_classes=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:40:26.836609Z","iopub.execute_input":"2025-09-08T12:40:26.836870Z","iopub.status.idle":"2025-09-08T12:40:27.081164Z","shell.execute_reply.started":"2025-09-08T12:40:26.836851Z","shell.execute_reply":"2025-09-08T12:40:27.080609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train(model, dataloader, epochs=1, lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:57:48.923620Z","iopub.status.idle":"2025-09-08T12:57:48.923857Z","shell.execute_reply.started":"2025-09-08T12:57:48.923752Z","shell.execute_reply":"2025-09-08T12:57:48.923762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T12:38:34.872527Z","iopub.execute_input":"2025-09-08T12:38:34.872795Z","iopub.status.idle":"2025-09-08T12:38:34.876442Z","shell.execute_reply.started":"2025-09-08T12:38:34.872774Z","shell.execute_reply":"2025-09-08T12:38:34.875693Z"}},"outputs":[],"execution_count":null}]}